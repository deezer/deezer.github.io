---
layout: post
title: "Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts"
date: 2022-12-04 10:00:00 +0100
category: Publication
author: kibrahim
readtime: 10
people:
 - kibrahim
 - eepure
publication_type: conference
publication_title: "Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts"
publication_year: 2022
publication_authors: K. M. Ibrahim, E. V. Epure, G. Peeters, G. Richard 
publication_journal: ISMIR
publication_preprint: "https://arxiv.org/pdf/2211.07250.pdf"
publication_code: "https://github.com/Karimmibrahim/Situational_Session_Generator"
---

As music has become more available especially on music streaming platforms, people have started to have distinct preferences to fit to their varying listening situations, also known as context. Hence, there has been a growing interest in considering the user's situation when recommending music to users. Previous works have proposed user-aware autotaggers to infer situation-related tags from music content and user's global listening preferences. However, in a practical music retrieval system, the autotagger could be only used by assuming that the context class is explicitly provided by the user. 

In this work, for designing a fully automatised music retrieval system, we propose to disambiguate the user's listening information from their stream data. Namely, we propose a system which can generate a situational playlist for a user at a certain time 1) by leveraging user-aware music autotaggers, and 2) by automatically inferring the user's situation from stream data (e.g. device, network) and user's general profile information (e.g. age). Experiments show that such a context-aware personalized music retrieval system is feasible, but the performance decreases in the case of new users, new tracks or when the number of context classes increases.

This paper has been published in the proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR 2022).